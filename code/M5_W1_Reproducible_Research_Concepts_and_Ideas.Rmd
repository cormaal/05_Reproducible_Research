---
title: "M5_W1_Reproducible_Research:_Concepts_and_Ideas"
author: "Alexander Cormack"
date: "2023-02-21"
output: html_document
---


# Reproducible Research: Concepts and Ideas


## What is Reproducible Research About

Not much to say here ... David makes the analogy with music and presents two pieces of music: a relatively simple 'pop' song and a quite complex piece of classical music.

The 'score' allows the conductor to reproduce the intention of the composer.

So for data analysis ... how do you develop the score for data analysis?

There is no real agreed upon 'score' for reproducing work ...

Some people will just describe in words what was done, and in some cases, this is sufficient, but in many cases, it's not.
Some people will provide the computer code and the data and everything that you need, and sometimes that's good.
But sometimes it's enormously complex and it's difficult to sort through.

And so there are a variety of ways that you can communicate data analysis, but we just haven't agreed upon what is a way that is more or less sufficient for everybody.

So, what we're gonna focus on in this course is how to communicate data analysis, using code by writing documents that are very dynamic,
and by sharing data so that other people can reproduce the work that you're doing. 


## Concepts and Ideas (Part 1)

### Replication

The ultimate standard for strengthening scientific evidence is replication of findings and conducting studies with independent:

- investigators
- data
- analytical methods
- laboratories
- instruments

Replication is particularly important in studies that can impact broad policy or regulatory decisions

### What's Wrong with replication?

Some studies cannot be replicated

- no time, opportunistic
- no money
- unique

Reproducible research: make analytical data and code available so that others may reproduce findings

How can we bridge the gap between replication and doing nothing? Reproducibility is the answer.

### Why do we need reproducible research?

- New technologies increasing data collection throughput; data are more complex and extremely high dimensional

- Existing databases can be merged into new 'megadatabases'

- Computing power is greatly increased, allowing more sophisticated analyses

- For every field 'X' there is a field 'Computational X'

### Example - reproducible air pollution and health research

- Estimating small (but important) health effects in the presence of much stronger signals
- Results inform substantial policy decisions, affect many stakeholders
    - EPA regulations can cost billions of dollars
- Complex statistical methods are needed and subject to intense scrutiny

Johns Hopkins have created the Internet-based Health and Air Pollution Surveillance System (iHAPSS) so their research can be reproduced by others


## Concepts and Ideas (Part 2)

David mentions the research pipeline and how the researcher and the reader try to meet in the middle.

Mentions that there has been a growing interest in reproducibility as evidenced by journal articles and such

### The Institute of Medicine report

In the discovery/test validation stage of omics-based tests:

- data/metadata used to develop tests should be made publicly available
- the computer code and fully specified computational procedures used for development of the candidate omics-based test should be made sustainably available
- *Ideally, the computer code that is released will* ***encompass all of the steps of computational analysis,*** *including all data preprocessing steps, that have been described in this chapter. All aspects of the analysis need to be transparently reported.*

### What do we need for reproducible research?

- Analytic data are available
- Analytic code are available
- Documentation of code and data
- Standard means of distribution

### Who are the players?

- Authors
    - want to make research reproducible
    - want tools for reproducible research to make their lives easier (or at leas not much harder)

- Readers
    - want to reproduce (and perhaps expand upon) interesting findings
    - want tools for reproducible research to make their lives easier

### Challenges

- Authors must undertake considerable effort to put data/results on the web (may not have resources like a web server)

- Readers must download data/results individually and piece together which data go with which code sections, etc.

- Readers may not have the same resources as authors

- Few tools to help authors/readers (although the toolbox is growing)

### In reality ...

- Authors
    - just put stuff on the web
    - (infamous) journal supplementary materials
    - there are some central databases for various fields (e.g. biology, ICPSR)

- Readers
  - just download the data and (try to) figure it out
  - piece together the software to run it


## Concepts and Ideas (Part 3)

### Literate statistical programming

- An article is a stream of text and code
- Analysis code is divided into text and code "chunks"
- Each code chunk loads data and computes results
- Presentation code formats results (tables, figures, etc.)
- Article text explains what is going on
- Literate programs can be *weaved* to produce human-readable documents and *tangled* to produce machine-readable documents

- Literate programming is a general concept that requires
    1. A documentation language (human readable)
    2. A programming language (machine readable)
- Sweave uses Latex and R as the documentation and programming languages
- Sweave was developed by Friedrich Leisch (member of the R Core) and is maintained by R core
- Main website: http://www.statistik.lmu.de/

- Sweave limitations
    - Focused primarily on Latex, a difficult to learn markup language used only by weirdos
    - Lacks features like caching, multiple plots per chunk, mixing programming languages and many other technical items
    - Not frequently updated or actively developed

- knitr is an alternative package
- Brings together many features added on to Sweave to address limitations
- knitr uses R as the programming language (although others are allowed) and a variety of documentation languages
    - Latex, Markdown, HTML
- knitr was developed by Yihui Xie (while a graduate student in statistics at Iowa State)
- See: http://yihui.name/knitr/

### Summary

- Reproducible research is important as a *minimum standard*, particularly for studies that are difficult to replicate
- Infrastructure is needed for *creating* and *distributing* reproducible documents, beyond what is currently available
- There is a growing number of tools for creating reproducible documents


## Scripting Your Analysis

Not much to say here ... David makes the analogy (again) with music and describes how the score is the document that tells all of the players what they have to play and how and when.

When doing data analysis you should write a script that describes all of the steps that were taken to reach the final outcome.


## Structure of a Data Analysis (Part 1)

### Steps in a data analysis

- define the question
- define the ideal data set
- determine what data you can access
- obtain the data
- clean the data
- exploratory data analysis
- statistical prediction/modeling
- interpret results
- challenge results
- synthesize/write up results
- create reproducible code

### The key challenge in data analysis

This was well summed up by the mathematics educator, Dan Myer, in a Ted talk:

*Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew all of the given information in advance? Where you didn't have a surplus of information and had to filter it out, or you had insufficient information and had to go find some?*

### Defining a question

The more effort you put into defining a reasonable question, the less effort you will have to spend filtering stuff.

An example

***Start with a general question***

Can I automatically detect emails that are SPAM or are not SPAM?

***Make it concrete***

Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?

### Defiine the ideal data set

The data set may depend on your role

- descriptive - a whole population
- exploratory - a random sample with many variables measured
- inferential - the right population, randomly sampled
- predictive - a training and test data set from the same population
- causal - data from a randomised study
- mechanistic - data about all components of the system

### Determine what data you can access

- Sometimes you can find data free on the web
- other times you may need to buy the data
- be sure to respcet the terms of use
- if the data don't exist, you may need to generate them yourself

### Obtain the data

- Try to obtain the raw data
- Be sure to reference the source
- Polite emails go a long way
- If you load the data from an internet source, record the url and time accessed

### Clean the data

- Raw data often needs to be processed
- If it is pre-processed, make sure you understand how
- Understand the source of the data (census, sample, convenience sample, etc.)
- May need reformatting, subsampling - record these steps
- **Determine if the data are good enough** - if not, quit or change data

### Our cleaned data set

We are going to try to answer the question posed above: 

*Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?*

We have decided to use the data set contained in the kernlab package

http://search.r-project.org/library/kernlab/html/spam.html

```{r}
# If it isn't installed, install the kernlab package with install.packages()
library(kernlab)
data(spam)
str(spam[, 1:5])
```

## Structure of Data Analysis (Part 2)

### Subsampling our data set

We need to generate a test and training set (prediction). The idea is that we are going to use a part of the data set to build our model, and then we are going to use another part of the data set which is independent of the first part actually to determine how good our model is at making predictions.

```{r}
library(kernlab)
data(spam)

# Perform the subsampling
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)
```
```{r}
trainSpam = spam[trainIndicator == 1, ]
testSpam = spam[trainIndicator == 0, ]
```

### Exploratory data analysis

- Look at summaries of the data
- Check for missing data
- Create exploratory plots
- Perform exploratory analyses (e.g. clustering)

```{r}
names(trainSpam)
```

```{r}
head(trainSpam)
```

```{r}
table(trainSpam$type)
```

```{r}
plot(trainSpam$capitalAve ~trainSpam$type)
```

```{r}
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type) # add 1 because there are many zeroes in the data
```

### Relationships between predictors

```{r}
plot(log10(trainSpam[, 1:4] +1))
```

### Clustering

```{r}
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
```

### New clustering

```{r}
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated)
```

### Statistical prediction / modeling

- Should be informed by the results of exploratory analysis
- Exact methods depend on the question of interest
- Transformations/processing should be accounted for when necessary
- Measures of uncertainty should be reported

```{r}
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y > 0.5))
cvError = rep(NA, 55)
library(boot)
for (i in 1:55) {
  lmFormula = reformulate(names(trainSpam)[i], response = "numType")
  glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
  cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}

## Which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)]
```

### Get a measure of uncertainty

```{r}
## Use the best model from the group
predictionModel = glm(numType ~ charDollar, family = "binomial", data = trainSpam)

## Get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam", dim(testSpam)[1])

## Classify as 'spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5] = "spam"
```
```{r}
## Classification table
table(predictedSpam, testSpam$type)
```
```{r}
## Error rate
(61 +458)/(1346 + 458 + 61 + 449)
```

### Interpret results

- Use appropriate language
    - describes
    - correlates with / associated with
    - leads to / causes
    - predicts
- Give an explanation
- Interpret coefficients
- Interpret measures of uncertainty

### Our example

- The fraction of characters that are dollar signs can be used to predict if an email is Spam
- Anything with more than 6.6% dollar signs is classified as Spam
- More dollar signs always means more Spam under our prediction
- Our test set error rate was 22.4%

### Challenge results

- Challenge all steps
    - question
    - data source
    - processing
    - analysis
    - conclusions
- Challenge measures of uncertainty
- Challenge choices of terms to include in models
- Think of potential alternative analyses

### Synthesise / write up results

- Lead with the question
- Summarise the analyses into the story
- Don't include every analysis, include it
    - if it is needed
    - if it is needed to address a challenge
- Order analyses according to the story, rather than chronologically
- Include 'pretty' figures that contribute to the story

### In our example

- Lead with the question
    - Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
- Describe the approach
    - Collected data from UCI -> created training / test sets
    - Explored relationships
    - Chose logistic model on training set by cross validation
    - Applied to test, 78% test set accuracy
- Interpret results
    - Number of dollar signs seems reasonable, e.g. "Make money with Viagra $ $ $ $!"
- Challenge results
    - 78% isn't that great
    - I could use more variables
    - Why logistic regression?

### Create reproducible code

Use R Markdown and knitr to document your code as you go along


## Organising Your Analysis

### Data analysis files

- Data
    - raw data
    - processed data
- Figures
    - exploratory figures
    - final figures
- R code
    - raw / unused scripts
    - final scripts
    - R Markdown files
- Text
    - README files
    - text of analysis / report

### Raw data

- Should be stored in your analysis folder
- If accessed from the web, include url, description and date accessed in README

### Processed data

- Processed data should be named so it is easy to see which script generated the data
- The processing script - processed data mapping should occur in the README
- Processed data should be tidy

### Exploratory figures

- Figures made during the course of your analysis, not necessarily part of your final report
- The do not need to be "pretty"

### Final figures

- Usually a small subset of the original figures
- Axes / colours set to make the figure clear
- Possible multiple panels

### Raw scripts

- May be less commented (but comments help you!)
- May be multiple versions
- May include analyses that are later discarded

### Final scripts

- Clearly commented
    - Small comments liberally - what, when, why, how
    - Bigger commented blocks for whole sections
- Include processing details
- Only analyses that appear in the final write-up

### R Markdown files

- R Markdown files can be used to generate reproducible reports
- Text and R code are integrated
- Very easy to create in RStudio

### README files

- Not necessary if you use R Markdown
- Should contain step-by-step instructions for analysis
- Here is an example: https://github.com/jtleek/swfdr/blob/master/README.md

### Text of the document

- It should include a title, introduction (motivation), methods (statistics you used), results (including measures of uncertainty), and conclusions (including potential problems)
- It should tell a story
- *It should not include every analysis you performed*
- References should be included for statistical methods

### Further resources

Reproducible research and biostatistics

https://academic.oup.com/biostatistics/article/10/3/405/293660


Managing statistical analysis project guidelines and best practices

https://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/


Project template - a pre-organised set of files for data analysis

http://projecttemplate.net/